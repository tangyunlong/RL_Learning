{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec00c7b",
   "metadata": {},
   "source": [
    "# Dyna-Q 算法\n",
    "### 区别的：\n",
    "+ 基于模型的强化学习：对环境的状态转移概率和奖励函数进行建模。\n",
    "    eg：Dyna-Q\n",
    "+ 无模型的强化学习：根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计。\n",
    "    eg：SARSA, Q-Learning\n",
    " \n",
    " ### 补充的：\n",
    "+ 强化学习算法有两个重要的评价指标：\n",
    "一个是算法收敛后的策略在初始状态下的***期望回报***，\n",
    "另一个是***样本复杂度***，即算法达到收敛结果需要在真实环境中采样的样本数量。\n",
    "\n",
    "### 对比的：\n",
    "***基于模型***的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，***因此通常会比无模型的强化学习算法具有更低的样本复杂度***。但是，环境模型可能并不准确，不能完全代替真实环境，因此***基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。***\n",
    "\n",
    "### 算法宏观上：\n",
    "Dyna-Q 使用一种叫做 Q-planning 的方法来***基于模型***生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态，采取一个曾经在该状态下执行过的动作，通过模型得到转移后的状态以及奖励，并根据这个模拟数据，用 Q-learning 的更新方式来更新动作价值函数。\n",
    "\n",
    "---\n",
    "### 理解上：\n",
    "***类比于人类的学习方式--刻意练习***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c9f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,起点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c31f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''在 Q-learning 的代码上进行简单修改，实现 Dyna-Q 的主要代码'''\n",
    "class DynaQ:\n",
    "    \"\"\" Dyna-Q算法 \"\"\"\n",
    "    def __init__(self,\n",
    "                 ncol,\n",
    "                 nrow,\n",
    "                 epsilon,\n",
    "                 alpha,\n",
    "                 gamma,\n",
    "                 n_planning,\n",
    "                 n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "\n",
    "        self.n_planning = n_planning  #执行Q-planning的次数, 对应1次Q-learning\n",
    "        self.model = dict()  # 环境模型\n",
    "\n",
    "    def take_action(self, state):  # 选取下一步的操作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "\n",
    "    def q_learning(self, s0, a0, r, s1):\n",
    "        td_error = r + self.gamma * self.Q_table[s1].max(\n",
    "        ) - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_error\n",
    "\n",
    "    def update(self, s0, a0, r, s1):\n",
    "        self.q_learning(s0, a0, r, s1)\n",
    "        '''\n",
    "        根据字典的性质，若该数据本身存在于字典中，便不会再一次进行添加。\n",
    "        '''\n",
    "        self.model[(s0, a0)] = r, s1  # 将数据添加到模型中\n",
    "        for _ in range(self.n_planning):  # Q-planning循环\n",
    "            # 随机选择曾经遇到过的状态动作对\n",
    "            (s, a), (r, s_) = random.choice(list(self.model.items()))\n",
    "            self.q_learning(s, a, r, s_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27700721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynaQ_CliffWalking(n_planning):\n",
    "    ncol = 12\n",
    "    nrow = 4\n",
    "    env = CliffWalkingEnv(ncol, nrow)\n",
    "    epsilon = 0.01\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)\n",
    "    num_episodes = 300  # 智能体在环境中运行多少条序列\n",
    "\n",
    "    return_list = []  # 记录每一条序列的回报\n",
    "    for i in range(10):  # 显示10个进度条\n",
    "        # tqdm的进度条功能\n",
    "        with tqdm(total=int(num_episodes / 10),\n",
    "                  desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "                episode_return = 0\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n",
    "                    agent.update(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "                return_list.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                        '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d957b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "n_planning_list = [0, 2, 20]\n",
    "for n_planning in n_planning_list:\n",
    "    print('Q-planning步数为：%d' % n_planning)\n",
    "    time.sleep(0.5)\n",
    "    return_list = DynaQ_CliffWalking(n_planning)\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.plot(episodes_list,\n",
    "             return_list,\n",
    "             label=str(n_planning) + ' planning steps')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Dyna-Q on {}'.format('Cliff Walking'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34973244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
